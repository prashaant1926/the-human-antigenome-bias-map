name: Researcher

on:
  pull_request:
    types: [opened]
  pull_request_review_comment:
    types: [created]

jobs:
  hypothesis:
    if: startsWith(github.head_ref, 'hypothesis/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # fetches entire git history

      - name: Hypothesis
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about each hypothesis before writing.
            Use ultra-careful reasoning to generate meaningful scientific hypotheses.
            Once done:
            - Update section_notes/01-research-concept-direction.md with your findings. 
            - Update hypothesis.jsonl with your hypotheses.
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  lit-review:
    if: startsWith(github.head_ref, 'lit-review/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # fetches entire git history

      - name: Literature Review
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            ULTRA THINK DEEPLY AND THOROUGHLY about the research landscape.
            Use ultra-careful analysis for comprehensive paper review using Arxiv and other research sites. 
            Ensure your sources are from reputable journals, conferences, and institutions.
            Once done:
            - Update related_work/ with your various markdown notes that seem useful/interesting. 
            - Update section_notes/02-lit-review.md with your review. 
            - Update paper.jsonl and hypothesis.jsonl with your findings (at least 15 papers).
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  ideas:
    if: startsWith(github.head_ref, 'ideas/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # fetches entire git history

      - name: Experiment Ideas
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about experimental design.
            Use ultra-careful planning to design rigorous experiments that an AI agent could run.
            Check proposal.jsonl and run.jsonl to see what has been done in the past. 
            Update section_notes/03-experiment-ideas.md with your ideas. Update proposal.jsonl with your proposals.
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  data:
    if: startsWith(github.head_ref, 'data/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write  # Changed to write for PR updates
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true  # Enable LFS checkout
          fetch-depth: 0 # fetches entire git history
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Git LFS
        run: |
          git lfs install
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Ensure LFS tracking is set up for data files
          git lfs track "data/**"
          git lfs track "*.tar.gz"
          git lfs track "*.zip"
          git lfs track "*.7z"
          git add .gitattributes || true

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install download tools
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl git-lfs unzip tar gzip bzip2 p7zip-full aria2
          git lfs install

      - name: Find Datasets
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Bash,Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pip*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "python*|python3*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "wget*|curl*|aria2c*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "mkdir*|ls*|find*|du*|head*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "tar*|unzip*|7z*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "git*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "kaggle*", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            ⚠️ CRITICAL - MUST READ FIRST - GIT LFS CONFIGURATION ⚠️
            ================================================================
            THIS REPOSITORY USES GIT LFS FOR ALL DATA FILES
            
            FORBIDDEN ACTIONS:
            ❌ NEVER add data/ to .gitignore
            ❌ NEVER skip datasets due to size
            ❌ NEVER use regular git add for large files
            
            REQUIRED: Setup Git LFS BEFORE downloading:
            ```bash
            # Configure Git LFS first
            git lfs install
            git lfs track "data/**"
            git lfs track "*.tar.gz" "*.zip" "*.7z" "*.h5" "*.pkl" "*.npy" "*.npz"
            git add .gitattributes
            git commit -m "chore: Configure Git LFS for data files"
            ```
            
            AFTER DOWNLOADING, commit with LFS:
            ```bash
            # Remove any .gitignore entries that block data
            if grep -q "data/" .gitignore 2>/dev/null; then
              sed -i '/data\//d' .gitignore
              git add .gitignore
            fi
            
            # Stage and commit with LFS
            git add data/
            git commit -m "feat: Add datasets via Git LFS"
            git lfs push origin HEAD
            git push origin HEAD
            ```
            ================================================================
            
            You are a research assistant using scientific thinking and rigorous methodology.
            
            STEP 1 - CHECK EXISTING DATASETS FIRST:
            ========================================
            IMPORTANT: Before downloading ANY new datasets, thoroughly check what's already available!
            
            ```bash
            # Check if data folder exists and what's already in it
            if [ -d "data" ]; then
              echo "=== Existing Data Folder Contents ==="
              ls -la data/
              echo ""
              echo "=== Subdirectories ==="
              find data -type d -maxdepth 2
              echo ""
              echo "=== All Data Files ==="
              find data -type f -name "*" | head -50
              echo ""
              echo "=== Dataset Sizes ==="
              du -sh data/* 2>/dev/null || echo "No data files yet"
              echo ""
              echo "=== Total Data Size ==="
              du -sh data/
              
              # Check for dataset documentation
              if [ -f "data/README.md" ]; then
                echo ""
                echo "=== Existing Dataset Documentation ==="
                cat data/README.md
              fi
              
              # Check for dataset metadata files
              echo ""
              echo "=== Dataset Metadata Files ==="
              find data -name "*.json" -o -name "*.yaml" -o -name "*.yml" -o -name "README*" -o -name "*.txt" | head -20
            else
              echo "No data directory found - will create and populate with datasets"
              mkdir -p data
            fi
            ```
            
            STEP 2 - ANALYZE EXISTING DATASETS:
            ====================================
            If datasets already exist, analyze them thoroughly:
            
            ```python
            import os
            import json
            from pathlib import Path
            
            data_dir = Path('data')
            existing_datasets = []
            
            # Scan for existing datasets
            if data_dir.exists():
                for item in data_dir.iterdir():
                    if item.is_dir():
                        dataset_info = {
                            'name': item.name,
                            'path': str(item),
                            'files': [],
                            'total_size': 0
                        }
                        
                        # Get all files in dataset
                        for file in item.rglob('*'):
                            if file.is_file():
                                size = file.stat().st_size
                                dataset_info['files'].append({
                                    'name': file.name,
                                    'path': str(file.relative_to(data_dir)),
                                    'size': size
                                })
                                dataset_info['total_size'] += size
                        
                        if dataset_info['files']:
                            existing_datasets.append(dataset_info)
                            print(f"Found dataset: {dataset_info['name']}")
                            print(f"  Files: {len(dataset_info['files'])}")
                            print(f"  Size: {dataset_info['total_size'] / (1024*1024):.2f} MB")
            
            # Save inventory
            with open('data/existing_datasets.json', 'w') as f:
                json.dump(existing_datasets, f, indent=2)
            
            print(f"\nTotal existing datasets: {len(existing_datasets)}")
            ```
            
            STEP 3 - DETERMINE WHAT'S NEEDED:
            ==================================
            Based on the research goals and existing datasets:
            
            1. List all datasets already available
            2. Identify gaps in the current dataset collection
            3. Only download NEW datasets that are actually needed
            4. Avoid duplicating existing data
            
            THINK DEEPLY AND THOROUGHLY about data requirements and quality.
            Your job is to INTELLIGENTLY manage datasets - use existing ones when appropriate!
            
            CRITICAL: You must physically download actual data files into the data/ folder!
            
            IMPORTANT: You have FULL COMMAND EXECUTION permissions with autoApprove enabled!
            You can run ANY command including pip, python, wget, curl, mkdir, etc.
            
            SETUP INSTRUCTIONS:
            1. First install ALL necessary tools for downloading data:
               ```bash
               # Python packages for ML datasets
               pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
               pip install datasets transformers scikit-learn pandas numpy matplotlib
               pip install kaggle huggingface-hub requests beautifulsoup4 gdown
               pip install tensorflow tensorflow-datasets
               pip install openml lxml
               
               # Install additional download tools
               pip install wget py7zr rarfile
               ```
            
            2. Create the data folder structure:
               ```bash
               mkdir -p data
               mkdir -p data/raw
               mkdir -p data/processed
               ```
            
            3. Use MULTIPLE methods to find and download datasets:
               
               a) Use torchvision for computer vision datasets:
               ```python
               import torchvision.datasets as datasets
               # Download CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, ImageNet subsets, etc.
               ```
               
               b) Use Hugging Face datasets:
               ```python
               from datasets import load_dataset, list_datasets
               # Browse available datasets: print(list_datasets())
               # Download: dataset = load_dataset('dataset_name')
               ```
               
               c) Use TensorFlow Datasets:
               ```python
               import tensorflow_datasets as tfds
               # List all: tfds.list_builders()
               # Download: dataset = tfds.load('dataset_name', download=True)
               ```
               
               d) Direct downloads with wget/curl/aria2:
               ```bash
               # Use wget for direct downloads
               wget -P data/raw/ "https://example.com/dataset.zip"
               
               # Use curl for APIs
               curl -L -o data/raw/dataset.tar.gz "https://example.com/dataset.tar.gz"
               
               # Use aria2 for faster parallel downloads
               aria2c -x 16 -s 16 -d data/raw/ "https://example.com/large_dataset.zip"
               ```
               
               e) Download from Kaggle:
               ```bash
               # Set up Kaggle API credentials if available
               kaggle datasets download -d dataset-name -p data/raw/
               ```
               
               f) Use gdown for Google Drive:
               ```python
               import gdown
               gdown.download('https://drive.google.com/...', 'data/raw/dataset.zip')
               ```
            
            4. Create a comprehensive download script `data/download_all_datasets.py`:
               ```python
               import os
               import requests
               import zipfile
               import tarfile
               import gdown
               from pathlib import Path
               
               def download_with_progress(url, filepath):
                   """Download file with progress bar"""
                   response = requests.get(url, stream=True)
                   total = int(response.headers.get('content-length', 0))
                   with open(filepath, 'wb') as file:
                       downloaded = 0
                       for data in response.iter_content(chunk_size=1024):
                           downloaded += len(data)
                           file.write(data)
                           print(f"Downloaded {downloaded}/{total} bytes", end='\r')
               
               # Download various datasets
               datasets_to_download = [
                   # Add dataset URLs here
               ]
               
               for url in datasets_to_download:
                   filename = url.split('/')[-1]
                   download_with_progress(url, f'data/raw/{filename}')
               ```
            
            5. Extract and organize downloaded files:
               ```bash
               # Extract zip files
               unzip data/raw/*.zip -d data/processed/
               
               # Extract tar files
               tar -xzf data/raw/*.tar.gz -C data/processed/
               
               # Extract 7z files
               7z x data/raw/*.7z -o data/processed/
               ```
            
            6. Verify ALL downloads:
               ```bash
               # List all downloaded files with sizes
               ls -lah data/
               ls -lah data/raw/
               ls -lah data/processed/
               
               # Check total size of downloaded data
               du -sh data/
               du -sh data/raw/*
               du -sh data/processed/*
               
               # Count files
               find data/ -type f | wc -l
               ```
            
            7. Create comprehensive documentation in data/README.md showing:
               - Exact file paths and sizes
               - Download timestamps
               - Data statistics (number of samples, features, etc.)
               - Loading instructions with code examples
               - License information
            
            REQUIREMENTS:
            - Download AT LEAST 5-10 different datasets
            - Include various types: tabular, image, text, time-series
            - Total downloaded data should be substantial (at least 1GB if possible)
            - Try multiple download methods until successful
            - If one source fails, try alternative sources
            - Focus on publicly available research datasets
            
            Use Exa search to find dataset URLs, repositories, and download links.
            Search for: "dataset download URL", "public research datasets", "benchmark datasets", "open data repositories"
            
            VERIFY SUCCESS:
            - Run: `find data/ -type f -name "*" | head -20` to show downloaded files
            - Run: `du -sh data/` to show total size
            - Ensure data/README.md lists all downloaded datasets with their locations
            
            FINAL COMMIT (refer to LFS instructions at top):
            After all downloads complete, commit everything with Git LFS as shown at the top of these instructions.
            Remember: NEVER add data/ to .gitignore - we use Git LFS for large files.
            
            Once done:
            - Update section_notes/04-datasets.md with dataset analysis
            - Create data/README.md with complete dataset catalog
            - Ensure datasets are committed with Git LFS
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  run:
    if: startsWith(github.head_ref, 'run/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write  # Changed to write
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true  # Pull LFS files (datasets)
          fetch-depth: 0 # fetches entire git history
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Verify datasets availability
        run: |
          echo "Checking for datasets..."
          if [ -d "data" ]; then
            echo "Data directory found. Contents:"
            ls -la data/
            du -sh data/* 2>/dev/null || echo "No data files yet"
          else
            echo "No data directory found - will need to download datasets"
          fi

      - name: Install tools
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl git-lfs
          # Install comprehensive ML and data science packages
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install datasets transformers scikit-learn pandas numpy matplotlib seaborn
          pip install tensorflow tensorflow-datasets
          pip install jax jaxlib optax flax
          pip install xgboost lightgbm catboost
          pip install scipy statsmodels networkx
          pip install jupyterlab notebook ipython
          pip install tqdm wandb tensorboard mlflow
          pip install pytest pytest-cov black flake8
          # Ensure Git LFS is tracking data files
          git lfs pull

      - name: Run Experiment
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Bash,Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pip*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "python*|python3*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "ls*|mkdir*|find*|du*|cat*|head*|tail*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "git clone*|git pull*|git fetch*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "wget*|curl*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "cd*|pwd*|echo*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "cp*|mv*|rm*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "jupyter*|ipython*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pytest*|black*|flake8*", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            ⚠️ ULTRA THINKING MODE - THINK DEEPLY BEFORE ACTING ⚠️
            ================================================================
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about the experimental implementation.
            Use ULTRA-CAREFUL reasoning and execution at every step.
            
            CRITICAL DATA ACCESS - GIT LFS DATASETS:
            ----------------------------------------
            The data/ folder contains datasets stored with Git LFS. They are ALREADY downloaded!
            ```bash
            # Verify datasets are available
            ls -la data/
            du -sh data/*
            
            # Read dataset documentation
            cat data/README.md
            
            # Load data in Python
            import pandas as pd
            import numpy as np
            from pathlib import Path
            
            data_dir = Path('../../data')  # Relative from experiments/<exp_id>/code/
            # Example: df = pd.read_csv(data_dir / 'dataset_name/train.csv')
            ```
            
            REFERENCE IMPLEMENTATIONS FROM PUBLIC REPOS:
            --------------------------------------------
            You can clone and study public GitHub repositories for reference:
            ```bash
            # Clone repos to experiments/<exp_id>/references/
            mkdir -p experiments/<exp_id>/references
            cd experiments/<exp_id>/references
            
            # Clone public repos for implementation ideas
            git clone https://github.com/user/repo.git
            
            # Study their approaches but write YOUR OWN implementation
            ```
            
            Use Exa search to find:
            - State-of-the-art implementations
            - Baseline models
            - Evaluation metrics
            - Best practices
            
            EXPERIMENT EXECUTION REQUIREMENTS:
            ----------------------------------
            1. SELECT experiment from proposal.jsonl
            2. CREATE experiment structure:
               ```bash
               exp_id="exp_$(date +%Y%m%d_%H%M%S)"
               mkdir -p experiments/$exp_id/{code,data,results,references}
               ```
            
            3. PLAN thoroughly in experiments/<exp_id>/plan.md:
               - Hypothesis to test
               - Datasets to use (from data/ folder)
               - Baseline methods
               - Evaluation metrics
               - Expected outcomes
            
            4. IMPLEMENT in experiments/<exp_id>/code/:
               ```python
               # main.py - Main experiment script
               # utils.py - Helper functions
               # models.py - Model architectures
               # evaluation.py - Metrics and evaluation
               # visualize.py - Results visualization
               ```
            
            5. EXECUTE the experiment:
               ```bash
               cd experiments/<exp_id>/code
               python main.py --data_path ../../data/<dataset_name> \
                              --output_dir ../results \
                              --seed 42
               ```
            
            6. ANALYZE results:
               - Generate plots with matplotlib/seaborn
               - Calculate statistical significance
               - Compare with baselines
               - Save all outputs to experiments/<exp_id>/results/
            
            7. DOCUMENT in experiments/<exp_id>/results.md:
               - Methodology
               - Results (include tables and figures)
               - Analysis and interpretation
               - Limitations
               - Next steps
            
            PYTHON EXECUTION BEST PRACTICES:
            --------------------------------
            - Set random seeds for reproducibility
            - Use proper train/val/test splits
            - Implement early stopping
            - Log metrics with wandb/tensorboard/mlflow
            - Save model checkpoints
            - Generate visualizations
            - Run statistical tests
            
            COMMIT RESULTS:
            --------------
            ```bash
            git add experiments/<exp_id>/
            git commit -m "feat: Complete experiment <exp_id> - <brief description>
            
            Results:
            - Metric 1: X.XX
            - Metric 2: Y.YY
            - Key finding: ..."
            git push origin HEAD
            ```
            
            Once done:
            - Append results to section_notes/05-experiment-runs.md
            - Remove experiment from proposal.jsonl
            - Update run.jsonl with your results 
          mcp_config: |
            {
              "mcpServers": {
                "search": {
                  "command": "npx",
                  "args": ["-y", "mcp-remote", "https://mcp.exa.ai/mcp?exaApiKey=${{ secrets.EXA_API_KEY }}"]
                }
              }
            }
  analyze:
    if: startsWith(github.head_ref, 'analyze/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write  # Changed to write
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true  # Pull LFS files (datasets)
          fetch-depth: 0 # fetches entire git history
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install analysis tools
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl
          pip install matplotlib seaborn plotly pandas numpy scipy scikit-learn

      - name: Analyze Experiment
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Bash,Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" },
                    { "tool": "Bash", "pattern": "pip*", "decision": "allow" },
                    { "tool": "Bash", "pattern": "python*|python3*", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and rigorous methodology.
            
            THINK DEEPLY AND THOROUGHLY about the results and their implications.
            Use ultra-careful analysis. Check analyze.jsonl and experiments/ to see what has been done in the past.
            
            You have FULL COMMAND EXECUTION permissions! Install any analysis packages you need:
            ```bash
            pip install matplotlib seaborn plotly pandas numpy scipy scikit-learn
            ```
            
            Use Python with matplotlib, seaborn, or plotly to create visualizations if needed.
            Load any datasets from the data/ folder if you need to verify or reanalyze results.
            
            Given experiments/<exp_id>/, analyze the results in experiments/<exp_id>/results and experiments/<exp_id>/result.md.
            Once done:
            - Append your analysis to section_notes/06-experiment-analyses.md.
            - Update analyze.jsonl with your analysis. 
  paper-draft:
    if: startsWith(github.head_ref, 'paper-draft/') || startsWith(github.head_ref, 'all/')
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # fetches entire git history

      - name: Write Paper Draft
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
          allowed_tools: "Edit,Write,Task,mcp__search__web_search_exa"
          settings: |
            {
              "chat": {
                "tools": {
                  "autoApprove": true,
                  "permissions": [
                    { "tool": "Edit", "decision": "allow" },
                    { "tool": "Write", "decision": "allow" },
                    { "tool": "Task", "decision": "allow" },
                    { "tool": "mcp__search__web_search_exa", "decision": "allow" }
                  ]
                }
              }
            }
          direct_prompt: |
            You are a research assistant using scientific thinking and following NeurIPS paper writing guidelines.
            
            THINK DEEPLY AND THOROUGHLY about the research narrative and contributions.
            Use ultra-careful synthesis to write a comprehensive paper draft in Latex. Look at the section_notes/ for each crucial section: research concept, literature review, experiment ideas, experiment runs, experiment analyses. 
            Your Latex structure won't be exactly the same but be faithful to the content in these sections.
            Use paper.jsonl for further related work papers to cite.
            You should look at the experiments conducted too in experiments/<exp_id>/result.md to see results of conducted experiments. 
            Make sure to include at least 5 plots and tables (in pure Latex) from the experiments conducted. 
            CONSTANTLY double check that your Latex compiles and you are following NeurIPS paper writing guidelines.
            It should be no longer than 8 pages not including references.
            Once done:
            - Add your draft to the paper-drafts/ folder.
